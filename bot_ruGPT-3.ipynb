{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qve4jGeFTA41"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вариант с ИИ ruGPT-3 от Сбербанка**"
      ],
      "metadata": {
        "id": "sjbTaJLITB9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Загружаем модель ruGPT-3 Medium (от Сбербанка)\n",
        "chatbot = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"sberbank-ai/rugpt3medium_based_on_gpt2\",\n",
        "    tokenizer=\"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
        ")\n",
        "\n",
        "# База знаний FAQ\n",
        "hospice_faq = hospice_faq = {\n",
        "    # Организационные\n",
        "    \"как оформить документы?\": \"Для оформления документов нужны паспорт, полис ОМС и направление врача. Обратитесь к администратору с 9:00 до 18:00.\",\n",
        "    \"график работы\": \"Хоспис работает круглосуточно. Посещение родственников — с 8:00 до 20:00.\",\n",
        "    \"контакты\": \"Телефон администрации: +7 (XXX) XXX-XX-XX. Экстренная связь: +7 (YYY) YYY-YY-YY.\",\n",
        "\n",
        "    # Медицинские\n",
        "    \"какие обезболивающие используются?\": \"Мы применяем препараты по протоколу ВОЗ. Конкретный план обезболивания назначает врач.\",\n",
        "    \"можно ли получить психологическую помощь?\": \"Да, наш психолог работает ежедневно. Запишитесь через администратора.\",\n",
        "\n",
        "    # Эмоциональная поддержка (заготовки для сложных случаев)\n",
        "    \"как справиться с болью у ребенка?\": \"Рекомендуем techniques... [можно добавить ссылки на методики]\",\n",
        "    \"что сказать ребенку?\": \"Важно говорить честно, но бережно... [шаблонные советы]\",\n",
        "}\n",
        "\n",
        "def get_rugpt_response(user_question):\n",
        "    prompt = f\"\"\"\n",
        "    Ты — профессиональный психолог, работающий в хосписе для детей.\n",
        "    Отвечай кратко, деликатно и по делу. Не выдумывай факты.\n",
        "\n",
        "    Вопрос: {user_question}\n",
        "    Ответ:\n",
        "    \"\"\"\n",
        "    response = chatbot(prompt, max_length=150)[0][\"generated_text\"]\n",
        "    return response.split(\"Ответ:\")[-1].strip()  # Берём только ответ\n",
        "    response = chatbot(\n",
        "        prompt,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        do_sample=True\n",
        "    )\n",
        "    return response[0][\"generated_text\"]\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat(message: str):\n",
        "    lower_msg = message.lower()\n",
        "\n",
        "    # Проверка FAQ\n",
        "    if lower_msg in hospice_faq:\n",
        "        return {\"response\": hospice_faq[lower_msg]}\n",
        "\n",
        "    # Эмоциональные вопросы\n",
        "    emotional_keywords = [\"как справиться\", \"поддержка\", \"совет\", \"боль\", \"страх\"]\n",
        "    if any(word in lower_msg for word in emotional_keywords):\n",
        "        return {\"response\": get_gpt_response(f\"Как поддержать семью с тяжелобольным ребёнком? Вопрос: {message}\")}\n",
        "\n",
        "    return {\"response\": \"Извините, я не понял вопрос. Уточните, пожалуйста.\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjvfeo_rDczL",
        "outputId": "7947d767-9964-4cf8-c3a8-e978d31660e7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLKzA0gYrsbp",
        "outputId": "02fe51a3-b08e-4aaf-9a25-f275f93500bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "В начале курса лечения.\n"
          ]
        }
      ],
      "source": [
        "# Пример использования\n",
        "print(get_rugpt_response(\"когда можно посещать больных\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L42YXWvNWV8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# 1. Инициализация моделей\n",
        "chatbot = pipeline(\"text-generation\", model=\"sberbank-ai/rugpt3medium_based_on_gpt2\")\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# 2. База знаний\n",
        "hospice_faq = hospice_faq = {\n",
        "    # Организационные\n",
        "    \"как оформить документы?\": \"Для оформления документов нужны паспорт, полис ОМС и направление врача. Обратитесь к администратору с 9:00 до 18:00.\",\n",
        "    \"график работы\": \"Хоспис работает круглосуточно. Посещение родственников — с 8:00 до 20:00.\",\n",
        "    \"контакты\": \"Телефон администрации: +7 (XXX) XXX-XX-XX. Экстренная связь: +7 (YYY) YYY-YY-YY.\",\n",
        "    \"можно ли находиться с ребенком ночью\": \"Да, но требуется согласование с дежурным врачом.\",\n",
        "\n",
        "    # Медицинские\n",
        "    \"какие обезболивающие используются?\": \"Мы применяем препараты по протоколу ВОЗ. Конкретный план обезболивания назначает врач.\",\n",
        "    \"можно ли получить психологическую помощь?\": \"Да, наш психолог работает ежедневно. Запишитесь через администратора.\",\n",
        "\n",
        "    # Эмоциональная поддержка (заготовки для сложных случаев)\n",
        "    \"как справиться с болью у ребенка?\": \"Рекомендуем techniques... [можно добавить ссылки на методики]\",\n",
        "    \"что сказать ребенку?\": \"Важно говорить честно, но бережно... [шаблонные советы]\",\n",
        "}\n",
        "\n",
        "# 3. Подготовка векторов FAQ\n",
        "questions = list(hospice_faq.keys())\n",
        "X = vectorizer.fit_transform(questions)\n",
        "\n",
        "def find_similar_question(user_query, threshold=0.7):\n",
        "    \"\"\"Находит наиболее похожий вопрос в FAQ\"\"\"\n",
        "    query_vec = vectorizer.transform([user_query])\n",
        "    similarities = cosine_similarity(query_vec, X)\n",
        "    max_idx = np.argmax(similarities)\n",
        "    return questions[max_idx] if similarities[0, max_idx] > threshold else None\n",
        "\n",
        "def generate_contextual_response(query):\n",
        "    \"\"\"Генерация ответа с учётом специфики хосписа\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    [Контекст: Ты консультант в детском хосписе. Отвечай кратко, профессионально и с сочувствием.]\n",
        "    Вопрос: {query}\n",
        "    Ответ:\n",
        "    \"\"\"\n",
        "    response = chatbot(prompt, max_length=150, temperature=0.5)[0][\"generated_text\"]\n",
        "    return response.split(\"Ответ:\")[-1].strip()\n",
        "\n",
        "def process_message(user_query):\n",
        "    # Шаг 1: Поиск в FAQ\n",
        "    matched_question = find_similar_question(user_query.lower())\n",
        "    if matched_question:\n",
        "        return hospice_faq[matched_question]\n",
        "\n",
        "    # Шаг 2: Генерация ответа\n",
        "    return generate_contextual_response(user_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFml3go6DJhw",
        "outputId": "567fc38c-fa4f-46ce-d286-a1c269939a0f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from fastapi import FastAPI\n",
        "import os\n",
        "# from dotenv import load_dotenv\n",
        "\n",
        "# Загрузка переменных окружения (если есть)\n",
        "# load_dotenv()\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# 1. Инициализация модели ruGPT-3\n",
        "print(\"Загружаю модель ruGPT-3...\")\n",
        "chatbot = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"sberbank-ai/rugpt3medium_based_on_gpt2\",\n",
        "    tokenizer=\"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
        ")\n",
        "\n",
        "# 2. База знаний FAQ\n",
        "hospice_faq = {\n",
        "\n",
        "    # Организационные\n",
        "    \"как оформить документы?\": \"Для оформления документов нужны паспорт, полис ОМС и направление врача. Обратитесь к администратору с 9:00 до 18:00.\",\n",
        "    \"график работы\": \"Хоспис работает круглосуточно. Посещение родственников — с 8:00 до 20:00.\",\n",
        "    \"контакты\": \"Телефон администрации: +7 (XXX) XXX-XX-XX. Экстренная связь: +7 (YYY) YYY-YY-YY.\",\n",
        "    \"можно ли находиться с ребенком ночью\": \"Да, но требуется согласование с дежурным врачом.\",\n",
        "\n",
        "    # Медицинские\n",
        "    \"какие обезболивающие используются?\": \"Мы применяем препараты по протоколу ВОЗ. Конкретный план обезболивания назначает врач.\",\n",
        "    \"можно ли получить психологическую помощь?\": \"Да, наш психолог работает ежедневно. Запишитесь через администратора.\",\n",
        "\n",
        "    # Эмоциональная поддержка (заготовки для сложных случаев)\n",
        "    \"как справиться с болью у ребенка?\": \"Рекомендуем techniques... [можно добавить ссылки на методики]\",\n",
        "    \"что сказать ребенку?\": \"Важно говорить честно, но бережно... [шаблонные советы]\",\n",
        "}\n",
        "\n",
        "# 3. Подготовка системы поиска по FAQ\n",
        "questions = list(hospice_faq.keys())\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(questions)\n",
        "\n",
        "def find_in_faq(user_query, threshold=0.7):\n",
        "    \"\"\"Поиск похожего вопроса в базе FAQ\"\"\"\n",
        "    query_vec = vectorizer.transform([user_query])\n",
        "    similarities = cosine_similarity(query_vec, X)\n",
        "    max_idx = np.argmax(similarities)\n",
        "    return questions[max_idx] if similarities[0, max_idx] > threshold else None\n",
        "\n",
        "def generate_answer(query):\n",
        "    \"\"\"Генерация контекстного ответа\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    [Контекст: Ты консультант в детском хосписе. Отвечай кратко, профессионально и с сочувствием. Не выдумывай факты.]\n",
        "    Вопрос: {query}\n",
        "    Ответ:\n",
        "    \"\"\"\n",
        "    response = chatbot(\n",
        "        prompt,\n",
        "        max_length=150,\n",
        "        temperature=0.5,\n",
        "        top_k=40,\n",
        "        do_sample=True\n",
        "    )\n",
        "    return response[0][\"generated_text\"].split(\"Ответ:\")[-1].strip()\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_handler(user_message: str):\n",
        "    # 1. Пытаемся найти ответ в FAQ\n",
        "    matched_question = find_in_faq(user_message.lower())\n",
        "    if matched_question:\n",
        "        return {\n",
        "            \"source\": \"FAQ\",\n",
        "            \"answer\": hospice_faq[matched_question]\n",
        "        }\n",
        "\n",
        "    # 2. Генерируем ответ\n",
        "    return {\n",
        "        \"source\": \"ruGPT-3\",\n",
        "        \"answer\": generate_answer(user_message)\n",
        "    }\n",
        "\n",
        "# Для теста в консоли\n",
        "if __name__ == \"__main__\":\n",
        "    from fastapi.testclient import TestClient\n",
        "    client = TestClient(app)\n",
        "\n",
        "#     # Тестовые запросы\n",
        "test_questions = [\n",
        "    \"как подать документы?\",\n",
        "    \"можно ли остаться на ночь?\",\n",
        "    \"как сказать ребенку о болезни?\",\n",
        "    \"какое обезболивание используется?\"\n",
        "    ]\n",
        "\n",
        "for q in test_questions:\n",
        "    response = client.post(\"/chat\", json={\"user_message\": q})\n",
        "    print(f\"Вопрос: {q}\")\n",
        "    # print(f\"Ответ ({response.json()['source']}): {response.json()['answer']}\\n\")\n",
        "    print(chat_handler(q))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrQfpmLuZJer",
        "outputId": "c00fb851-f015-4243-e96d-fd38800f0493"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загружаю модель ruGPT-3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вопрос: как подать документы?\n",
            "<coroutine object chat_handler at 0x7db9d69203c0>\n",
            "Вопрос: можно ли остаться на ночь?\n",
            "<coroutine object chat_handler at 0x7db9d69203c0>\n",
            "Вопрос: как сказать ребенку о болезни?\n",
            "<coroutine object chat_handler at 0x7db9d69212a0>\n",
            "Вопрос: какое обезболивание используется?\n",
            "<coroutine object chat_handler at 0x7db9d6921460>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2-2258644503.py:101: RuntimeWarning: coroutine 'chat_handler' was never awaited\n",
            "  print(chat_handler(q))\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример использования\n",
        "print(chat_handler(\"когда можно посещать больных\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usSxcuFrWlc1",
        "outputId": "4a52e82e-51da-4217-9a28-0dff7942ed5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<coroutine object chat_handler at 0x7dba6a065620>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-4008307977.py:2: RuntimeWarning: coroutine 'chat_handler' was never awaited\n",
            "  print(chat_handler(\"когда можно посещать больных\"))\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1k_SlPEyfwmo"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch scikit-learn fastapi uvicorn nest-asyncio\n",
        "\n",
        "from transformers import pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from fastapi import FastAPI, Request\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from fastapi.testclient import TestClient\n",
        "\n",
        "# Инициализация модели\n",
        "print(\"Загружаю модель ruGPT-3...\")\n",
        "chatbot = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"sberbank-ai/rugpt3medium_based_on_gpt2\",\n",
        "    device=\"cpu\"\n",
        ")\n",
        "\n",
        "# База знаний FAQ\n",
        "hospice_faq = {\n",
        "    # Организационные\n",
        "    \"как оформить документы?\": \"Для оформления документов нужны паспорт, полис ОМС и направление врача. Обратитесь к администратору с 9:00 до 18:00.\",\n",
        "    \"график работы\": \"Хоспис работает круглосуточно. Посещение родственников — с 8:00 до 20:00.\",\n",
        "    \"контакты\": \"Телефон администрации: +7 (XXX) XXX-XX-XX. Экстренная связь: +7 (YYY) YYY-YY-YY.\",\n",
        "    \"можно ли находиться с ребенком ночью\": \"Да, но требуется согласование с дежурным врачом.\",\n",
        "\n",
        "    # Медицинские\n",
        "    \"какие обезболивающие используются?\": \"Мы применяем препараты по протоколу ВОЗ. Конкретный план обезболивания назначает врач.\",\n",
        "    \"можно ли получить психологическую помощь?\": \"Да, наш психолог работает ежедневно. Запишитесь через администратора.\",\n",
        "\n",
        "    # Эмоциональная поддержка (заготовки для сложных случаев)\n",
        "    \"как справиться с болью у ребенка?\": \"Рекомендуем techniques... [можно добавить ссылки на методики]\",\n",
        "    \"что сказать ребенку?\": \"Важно говорить честно, но бережно... [шаблонные советы]\",\n",
        "}\n",
        "\n",
        "# Инициализация FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "async def chat_handler(request: Request):\n",
        "    data = await request.json()\n",
        "    user_message = data.get(\"user_message\", \"\")\n",
        "\n",
        "    # Поиск в FAQ\n",
        "    matched_question = None\n",
        "    for question in hospice_faq:\n",
        "        if question.lower() in user_message.lower():\n",
        "            matched_question = question\n",
        "            break\n",
        "\n",
        "    if matched_question:\n",
        "        return {\"source\": \"FAQ\", \"answer\": hospice_faq[matched_question]}\n",
        "\n",
        "    # Генерация ответа\n",
        "    prompt = f\"Вопрос: {user_message}\\nОтвет:\"\n",
        "    response = chatbot(prompt, max_length=100)[0][\"generated_text\"]\n",
        "    answer = response.split(\"Ответ:\")[-1].strip()\n",
        "\n",
        "    return {\"source\": \"ruGPT-3\", \"answer\": answer}\n",
        "\n",
        "# Тестирование\n",
        "print(\"\\n=== Тестирование ===\")\n",
        "client = TestClient(app)\n",
        "\n",
        "test_cases = [\n",
        "    {\"user_message\": \"как оформить документы?\"},\n",
        "    {\"user_message\": \"Можно ли мне остаться с ребенком на ночь?\"},\n",
        "    {\"user_message\": \"Как сказать ребенку о болезни?\"},\n",
        "    {\"user_message\": \"Что делать при боли?\"}\n",
        "]\n",
        "\n",
        "for test in test_cases:\n",
        "    response = client.post(\"/chat\", json=test)\n",
        "    print(f\"Вопрос: {test['user_message']}\")\n",
        "    print(f\"Ответ: {response.json()}\\n\")\n",
        "\n",
        "# Запуск сервера\n",
        "print(\"\\n=== Запуск сервера ===\")\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "90VeRvYzf0Ae",
        "outputId": "df647a2f-d89a-4cbb-8501-d1090aeb61a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-480998265.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers torch scikit-learn fastapi uvicorn nest-asyncio'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     return {\n\u001b[1;32m   1086\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[0;32m--> 606\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_egginfo_installed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_egginfo_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36m_read_files_distinfo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    615\u001b[0m         \"\"\"\n\u001b[1;32m    616\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RECORD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_files_egginfo_installed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}